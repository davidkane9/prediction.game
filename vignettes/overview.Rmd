---
title: "Overview of the prediction.game Package"
output: rmarkdown::html_vignette
author: "David Kane"
vignette: >
  %\VignetteIndexEntry{Overview of the prediction.game Package}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

The purpose of this vignette is to provide an overview of the **prediction.game** package. I will also intersperse notes about future plans, for the reference of both potential collaborators and myself.

The only function is *play()* which allows for playing of the "prediction game," especially in class. Imagine that we are having a contest to determine who can come up with a number which is closest to a random draw from the *mpg* variable in the *mtcars* data frame. (There are `r length(mtcars$mpg)`  values for *mpg*, with a mean of `r round(mean(mtcars$mpg), 2)`.) 

```{r setup}
suppressMessages(library(tidyverse))
library(prediction.game)

play(n = 1000, guess_1 = 20.09, guess_2 = 19, formula = ~ sample(mtcars$mpg, size = 1)) %>% 
  count(winner)
```

You might think that the mean should win more contests than some other number but, as you can see above, 19 is a "better" guess, meaning that it wins more contests. The "trick" is that *mpg* has a right skew, with a median, at `r median(mtcars$mpg)`, lower than the mean. And, for getting closest to a random draw from a distribution, the median is better than the mean, when the two differ.

At the other extreme, we can "sample" the entire distribution.

```{r}
play(n = 1000, guess_1 = 20.09, guess_2 = 19, formula = ~ sample(mtcars$mpg, size = 1)) %>% 
  count(winner)
```



## Further Thoughts

Need a function, show(), which takes, via %>%, the result of play() and creates a cool animation using d3rain.

Should we connect all this to bootstrapping? What we have, implicitly (?), is a competition over sort-of bootstraped samples . . .

Should we connect all this to the concept to cross-validation? That is, the same (sort of?) framework as what we have here --- lots of experiments, each using a different sample of the data --- is like cross-validation. Instead of taking a sample, you divude the data into 10 sets training and test data, run the two competing processes each on the training, and then compare how they do on the test.

Issue 1: Need to be able to run contests in which the formula does not produce a single number. Or do we? Hmmm. 

Issue 2: No longer hard-code the evaluation function. Right now, we use the absolute value of the distance to evaluate the winner. But what if we wanted the squared distance? Or is this a bit of a trick question since the winner in both metrics is the same . . .

Big question: What if, instead of declaring a winner each draw, we want to sum up all the errors (absolute or squared or cubed or whatever) and then use that sum to determine the winner? This would suggest that, within an individual contest, we might compare our two guesses against a draw of 10 or 100 or whatever answers from the formula. Of course, we might still want to do that n times . . .

Bigger (?) question: Do we need for guesses to be formulas? Not today.

 
Calculate your guess however you want, using whatever statistical tricks you want. But, at that point, your guess is just as number, as is mine, and we are going to see who wins. Any randomness comes from the formula.
